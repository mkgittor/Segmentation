# -*- coding: utf-8 -*-
"""Test_Data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16tLl70ZsM4hJy9gLz_VHVvsc0HkcTb7f

### Importing data and basic Python libraries
"""

!pip install -q pandasql

import csv
import numpy as np
import math
import pandas as pd
import time
import pandasql as psql
import matplotlib.pyplot as plt 
import seaborn as sns

"""### Reading Accounts, Customers and Transactions columns into a dataframe, Data preparation"""

# Accounts dataframe obtained from CSV file

df_accts = pd.read_csv(r'BusinessCase_Accts.csv')
df_accts.shape[0]
#11,290 rows and 9 columns

#df_accts.rename(columns={'Unnamed: 0':'Index'},inplace=True)

# Connfirming if Index can be used as Index column

#df_accts['Index'].nunique()
# 11,290
#df_accts[['balance', 'Index']].groupby('Index').count()
# All index values have count 1



df_accts = pd.read_csv(r'BusinessCase_Accts.csv',index_col=0)
df_accts

# Customers dataframe obtained from CSV file

df_custs = pd.read_csv(r'BusinessCase_Custs.csv')
df_custs.shape[0]
#5645 rows and 13 columns

df_custs.rename(columns={'Unnamed: 0':'Index'},inplace=True)
df_custs
# Confirming if Index can be used as Index column

#df_custs['Index'].nunique()
# 5,645
df_custs[['birthDate', 'Index']].groupby('Index').count()
# All index values have count 1

df_custs = pd.read_csv(r'BusinessCase_Custs.csv',index_col=0)
df_custs
df_custs['id'].nunique()
# 5,645 unique customers

# Transactions dataframe obtained from CSV file

df_txn = pd.read_csv(r'BusinessCase_Tx.csv')
df_txn.shape[1]
#92,304 rows and 10 columns
df_txn
df_txn.rename(columns={'Unnamed: 0':'Index'},inplace=True)
df_txn
# Confirming if Index can be used as Index column

df_txn['Index'].nunique()
# 92,304
df_txn[['currencyAmount', 'Index']].groupby('Index').count()
# All index values have count 1

df_txn = pd.read_csv(r'BusinessCase_Tx.csv',index_col=0)
#df_txn
#df_custs
#df_custs.dtypes

# Deriving customer Age (years and months) in Customers Dataframe

df_custs['Age_jul2019_inmonths'] = (pd.to_datetime('2019-07-01') - pd.to_datetime(df_custs['birthDate'])).astype('<m8[M]')
df_custs['Age_jul2019_years'] = ((pd.to_datetime('2019-07-01') - pd.to_datetime(df_custs['birthDate'])).astype('<m8[M]')/12).apply(np.floor)
df_custs['Age_jul2019_mths'] = ((pd.to_datetime('2019-07-01') - pd.to_datetime(df_custs['birthDate'])).astype('<m8[M]'))-df_custs['Age_jul2019_years']*12

#Merging customers and accounts dataframes

df_custs_accts = pd.merge(left=df_custs,right=df_accts, how='left', left_on='id', right_on='cust_id')

# Deriving Year-month and date variable in txn dataframe

df_txn['yearmonth'] = pd.to_datetime(df_txn['originationDateTime']).apply(lambda x: x.strftime('%Y%m')) 
df_txn['date_YMD'] = pd.to_datetime(df_txn['originationDateTime']).apply(lambda x: x.strftime('%Y-%m-%d')) 
df_txn.dtypes

# Filtered transaction data for Starbucks

df_txn_strbcks = df_txn[['STARBUCKS' in x for x in df_txn['description']]]

df_txn_daily=df_txn_strbcks.groupby(['yearmonth','date_YMD'])['currencyAmount'].sum().reset_index()

# 10 point Moving average based on daily spend in select months of 2018

df_txn_daily['MA_10pt'] = df_txn_daily.currencyAmount.rolling(window=10).mean()

df_txn_daily_10ptMA = df_txn_daily.fillna(0)

df_custs_accts.totalIncome.describe()

"""### Answers to queries - Summaries, Hypothesis Testing"""

# 1 : most number of customers?

q1 = """ select a.branchNumber, a.customer_count from 
(select branchNumber, count(distinct(cust_id)) as customer_count from df_accts group by branchNumber) a
inner join 
(select max(customer_count) as max_count from (select branchNumber, count(distinct(cust_id)) as customer_count from df_accts group by branchNumber)) b
on a.customer_count=b.max_count"""

print(psql.sqldf(q1,locals()))
# Branch number 1029 has 151 customers

# 2 : oldest customer as of 2019-07-01?

q2 = """ select a.birthDate, a.Age_jul2019_years,a.Age_jul2019_mths from 
(select * from df_custs) a
inner join 
(select max(Age_jul2019_inmonths) as max_age from (select * from df_custs)) b
on a.Age_jul2019_inmonths=b.max_age"""
print(psql.sqldf(q2,locals()))
# Age of oldest customer as of Jul 1st, 2019 is 106 Years 8 Months

# 3 : accounts with the oldest customer ?

q3 = """ select birthDate,Age_jul2019_years,Age_jul2019_mths,acct_count from 
(select cust_id,birthDate,Age_jul2019_inmonths,Age_jul2019_years,Age_jul2019_mths,count(type_y) as acct_count from df_custs_accts group by cust_id,birthDate,Age_jul2019_inmonths,Age_jul2019_years,Age_jul2019_mths) a
inner join 
(select max(Age_jul2019_inmonths) as max_age from (select * from df_custs_accts)) b
on a.Age_jul2019_inmonths=b.max_age"""
print(psql.sqldf(q3,locals()))
# Oldest customer has 2 accounts

# 4,5 : # transactions went to select merchant in April, How much was spent on them merchant in April

q4 = """ select yearmonth,
count(distinct case when currencyAmount>=0 then date_YMD else NULL end) as txn_day,
sum(case when currencyAmount>0 then 1 else 0 end) as txn_count,
sum(case when currencyAmount=0 then 1 else 0 end) as txn0,
sum(case when currencyAmount<0 then 1 else 0 end) as txnLT0,
sum(case when (currencyAmount='.' or trim(currencyAmount)  in ('',' ')) then 1 else 0 end) as txnmis,
sum(case when currencyAmount>0 then currencyAmount else 0 end) as amount_spent,
round(avg(currencyAmount),2) as avg_txn_amnt 
from df_txn where (description like '%starbucks%') group by yearmonth """
print(psql.sqldf(q4,locals()))

# 6 : 

#Normality check for April and June month's starbucks daily spend

from statsmodels.graphics.gofplots import qqplot
from matplotlib import pyplot

# q-q plot for Apr 2018
qqplot(df_txn_daily.loc[df_txn_daily['yearmonth'].isin({'201804'})].currencyAmount, line='s')
pyplot.show()

# normality test Shapiro-Wilk for Apr 2018
from scipy.stats import shapiro

stat04, p04 = shapiro(df_txn_daily.loc[df_txn_daily['yearmonth'].isin({'201804'})].currencyAmount)
print('Stat04=%.3f, p04=%.3f' % (stat04, p04))

# q-q plot for June 2018
qqplot(df_txn_daily.loc[df_txn_daily['yearmonth'].isin({'201806'})].currencyAmount, line='s')
pyplot.show()

# normality test Shapiro-Wilk for June 2018
stat06, p06 = shapiro(df_txn_daily.loc[df_txn_daily['yearmonth'].isin({'201806'})].currencyAmount)
print('stat06=%.3f, p06=%.3f' % (stat06, p06))

#Equal variance check for April and June month's starbucks daily spend

import scipy.stats as stats

F = max(np.var(df_txn_daily.loc[df_txn_daily['yearmonth'].isin({'201804'})].currencyAmount),np.var(df_txn_daily.loc[df_txn_daily['yearmonth'].isin({'201806'})].currencyAmount))/min(np.var(df_txn_daily.loc[df_txn_daily['yearmonth'].isin({'201804'})].currencyAmount),np.var(df_txn_daily.loc[df_txn_daily['yearmonth'].isin({'201806'})].currencyAmount))
df1 = len(df_txn_daily.loc[df_txn_daily['yearmonth'].isin({'201804'})].currencyAmount)-1
df2 = len(df_txn_daily.loc[df_txn_daily['yearmonth'].isin({'201806'})].currencyAmount)-1
p_value = 1-stats.f.cdf(F, df1, df2)
print('Equal Variance p_value=%.3f' % (p_value))

# p value(0.39) > Alpha(0.05) meaning we cannot reject the null hypothesis that variance is equal

q6_pre = """ select count(distinct(date_YMD)) as dist_dates_jun18,
min(date_YMD) as min_date_jun18,
max(date_YMD) as max_date_jun18
from df_txn_strbcks where yearmonth in (201806) order by dist_dates_jun18"""

print(psql.sqldf(q6_pre,locals()))

q6 = """ select 
count(distinct case when yearmonth = 201804 then date_YMD end) as dapr18,
count(distinct case when yearmonth = 201804 then customerId end) as cust_apr18,
count(distinct case when yearmonth = 201806 then date_YMD end) as djun181,
count(distinct case when yearmonth = 201806 then customerId end) as cust_jun181,
count(distinct case when (yearmonth = 201806 and date_YMD not in ('2018-06-29')) then date_YMD end) as djun182,
count(distinct case when (yearmonth = 201806 and date_YMD not in ('2018-06-29')) then customerId end) as cust_jun182,
count(distinct case when (yearmonth = 201806 and date_YMD not in ('2018-06-29')) and customerId in 
(select distinct(customerId) from df_txn_strbcks where yearmonth in (201804)) then customerId end) as comcust
from df_txn_strbcks where yearmonth in (201804,201806)"""

print(psql.sqldf(q6,locals()))

# 147 (78%) of 189 customers from first 16 days in June 18 are repeat customers from total 16 days in Apr 18 

#t test for normaly distributed equal variance dependent samples

from math import sqrt
from numpy import mean
from scipy import stats

data1 = df_txn_daily.loc[df_txn_daily['yearmonth'].isin({'201804'})].currencyAmount
data2 = df_txn_daily.sort_values('date_YMD', ascending=True).loc[df_txn_daily['yearmonth'].isin({'201806'})].currencyAmount.head(16)
stat, dep_p = stats.ttest_rel(data1, data2)
print('stat=%.3f, dep_p=%.3f' % (stat, dep_p))

# p value (0.400) > alpha (0.05) meaning we cannot statistically reject the Null hypothesis that 
#the daily average spend at Starbucks is similar in April 2018 and Jun 2018

# 7 : Moving average calculation

q7 = """ select a.yearmonth,a.date_YMD,a.currencyAmount,a.MA_10pt,spendMA_diff from 
(select yearmonth,date_YMD,currencyAmount,MA_10pt, case when MA_10pt<>0 then (currencyAmount-MA_10pt) else 0 end as spendMA_diff from df_txn_daily_10ptMA) a
inner join 
(select max(spendMA_diff) as max_diff from (select yearmonth,date_YMD,currencyAmount,MA_10pt, case when MA_10pt<>0 then (currencyAmount-MA_10pt) else 0 end as spendMA_diff from df_txn_daily_10ptMA)) b
on a.spendMA_diff=b.max_diff"""

print(psql.sqldf(q7,locals()))

"""### Data Quality, Exploration"""

# Data Exploration - Missing Values


def missing_zero_values_table(df):
        zero_val = (df == 0.00).astype(int).sum(axis=0)
        mis_val = df.isnull().sum()
        mis_val_percent = 100 * df.isnull().sum() / len(df)
        mz_table = pd.concat([zero_val, mis_val, mis_val_percent], axis=1)
        mz_table = mz_table.rename(
        columns = {0 : 'Zero Values', 1 : 'Missing Values', 2 : '% of Total Values'})
        mz_table['Total Zero Missing Values'] = mz_table['Zero Values'] + mz_table['Missing Values']
        mz_table['% Total Zero Missing Values'] = 100 * mz_table['Total Zero Missing Values'] / len(df)
        mz_table['Data Type'] = df.dtypes
        mz_table = mz_table[
            mz_table.iloc[:,1] != 0].sort_values(
        '% of Total Values', ascending=False).round(1)
        print ("Your selected dataframe has " + str(df.shape[1]) + " columns and " + str(df.shape[0]) + " Rows.\n"      
            "There are " + str(mz_table.shape[0]) +
              " columns that have missing values.")
        return mz_table

missing_zero_values_table(df_txn)

# df_accts missing %: iban - 100%
# df_custs misisng % : schools - 93% ; schoolAttendance - 72% ; workactivity - 25.6% ; HabitationStatus - 17.4% ; occupationIndustry - 12.9%
# df_txn misssing % : locationRegion - 100% ; locaationCity - 100% ; merchantId - 53.1% ; categoryTags - 0.2%

# Income distribution of customers 

df_custs.totalIncome.quantile(np.linspace(.1, 1, 9, 0))


q8 = """ select currency,type, count(1) as rec_count, count(distinct cust_id) as dist_cust from df_accts group by currency,type """

print(psql.sqldf(q8,locals()))
# All customers have a DDA and SDA type in df_accts, currency is always CAD

q9 = """ select addresses_principalResidence_province as adrs, workactivity as wrkact, schoolAttendance as schatt, case when Age_jul2019_years<25 then 1 else 0 end as agchk, count(1) as rec_count from df_custs group by addresses_principalResidence_province, workactivity, schoolAttendance,case when Age_jul2019_years<25 then 1 else 0 end """ 

print(psql.sqldf(q9,locals()))

q10 = """ select HabitationStatus as hbtst, workactivity as wrkact, count(1) as rec_count from df_custs group by HabitationStatus, workactivity """ 

print(psql.sqldf(q10,locals()))

q11 = """ select occupationIndustry as occind, count(1) as rec_count from df_custs group by occupationIndustry order by rec_count desc""" 

print(psql.sqldf(q11,locals()))


#df_txn['customerId'].nunique()
#df_txn['accountId'].nunique()
# 5,483 customers' transactions from 7,475 unique accounts in df_txn dataframe

q12 = """ select case when Age_jul2019_years<18 then 'lt18'
when Age_jul2019_years>=18 and Age_jul2019_years<30 then '18-29'
when Age_jul2019_years>=30 and Age_jul2019_years<45 then '30-44'
when Age_jul2019_years>=45 and Age_jul2019_years<60 then '45-59'
else '>=60' end as age_group,case when totalIncome < 13000 then 'lt13k'
when totalIncome>=13000 and totalIncome<30000 then '13k-30k'
when totalIncome>=30000 and totalIncome<50000 then '30k-50k'
when totalIncome>=50000 and totalIncome<75000 then '50k-75k'
else 'ge75k' end as inc_brk, count(distinct(id_x)) as notxn_cust from df_custs_accts where id_x not in 
(select distinct customerId from df_txn)
group by case when Age_jul2019_years<18 then 'lt18'
when Age_jul2019_years>=18 and Age_jul2019_years<30 then '18-29'
when Age_jul2019_years>=30 and Age_jul2019_years<45 then '30-44'
when Age_jul2019_years>=45 and Age_jul2019_years<60 then '45-59'
else '>=60' end,
case when totalIncome < 13000 then 'lt13k'
when totalIncome>=13000 and totalIncome<30000 then '13k-30k'
when totalIncome>=30000 and totalIncome<50000 then '30k-50k'
when totalIncome>=50000 and totalIncome<75000 then '50k-75k'
else 'ge75k' end """

print(psql.sqldf(q12,locals()))

# 162 no transaction customers

q13 = """ select type_y, count(1) as rec_count, min(openDate) as min_opn_dt, max(openDate) as mx_opn_dt from df_custs_accts group by type_y """
print(psql.sqldf(q13,locals()))


# DQ queries

q14 = """ select a.categoryTags,min(a.currencyAmount) as min_spend,
max(a.currencyAmount) as max_spend,
sum(case when currencyAmount=0 then 1 else NULL end) as Num_0_txn,
sum(case when currencyAmount>0 then 1 else NULL end) as Num_non0_txn
from df_txn a group by a.categoryTags """

print(psql.sqldf(q14,locals()))

#Data QA on March month

q16 = """ select yearmonth,
count(distinct case when currencyAmount>0 then date_YMD else NULL end) as txn_day,
count(distinct case when currencyAmount>0 then customerId else NULL end) as cust_txn,
count(distinct case when yearmonth=201803 and customerId not in (select distinct(customerId) from df_txn
where yearmonth in (201804,201805,201806,201807,201808,201809,201810)) then customerId else NULL end) as onlyMrccust,
sum(case when currencyAmount>0 then 1 else 0 end) as txn_count,
sum(case when currencyAmount=0 then 1 else 0 end) as txn0,
round(avg(currencyAmount),2) as avg_txn_amnt 
from df_txn group by yearmonth """
print(psql.sqldf(q16,locals()))


# March month has very low transactions and customers with transaction

q17 = """ select a.currencyAmount,a.categoryTags,a.description from df_txn a
where (yearmonth=201803 and customerId not in (select distinct(customerId) from df_txn
where yearmonth in (201804,201805,201806,201807,201808,201809,201810))) """

print(psql.sqldf(q17,locals()))

q18 = """ select categoryTags,
count(distinct case when currencyAmount>0 then date_YMD else NULL end) as txn_day,
count(distinct case when currencyAmount>0 then customerId else NULL end) as cust_txn,
count(distinct case when yearmonth=201803 and customerId not in (select distinct(customerId) from df_txn
where yearmonth in (201804,201805,201806,201807,201808,201809,201810)) then customerId else NULL end) as onlyMrccust,
sum(case when currencyAmount>0 then 1 else 0 end) as txn_count,
round(avg(currencyAmount),2) as avg_txn_amnt 
from df_txn where yearmonth=201803 group by categoryTags """
print(psql.sqldf(q18,locals()))


# Missing Merchant Categorytags in March month

df_txn['merchcateg_txnflag'] = np.where(df_txn['currencyAmount']>0, 1, 0)

df_txn['datetime_form'] = pd.to_datetime(df_txn['originationDateTime']).dt.hour
#df_txn.dtypes

def f(x):
    if (x > 4) and (x < 12):
        return '1'
    elif (x >= 12) and (x <= 17):
        return'2'
    elif (x > 17) and (x<=20):
        return'3'
    elif (x > 20) and (x<=24):
        return'4'
    elif (x<=4):
        return'5'

df_txn['session1'] = df_txn['datetime_form'].apply(f)


# Considering Month from Apr 2018 onwards only for further deep dive with positive transaction value

df_txn_apr_Oct18 = df_txn.loc[(df_txn['yearmonth'].isin({'201804','201805','201806','201807','201808','201809','201810'}))]

q19 = """ select yearmonth,
count(distinct case when currencyAmount>=0 then date_YMD else NULL end) as txn_day,
count(distinct case when currencyAmount>=0 then customerId else NULL end) as cust_txn,
sum(case when currencyAmount>=0 then 1 else 0 end) as txn_count,
sum(case when currencyAmount=0 then 1 else 0 end) as txn0,
round(avg(currencyAmount),2) as avg_txn_amnt 
from df_txn_apr_Oct18 group by yearmonth """
print(psql.sqldf(q19,locals()))


#Creating an index variable from date of transaction

df_txn_apr_Oct18['indexvar'] = pd.to_datetime(df_txn_apr_Oct18['date_YMD'])
df_txn.dtypes

# Aggregating at daily level 

df_txn_daily_allmerch=df_txn_apr_Oct18.groupby(['yearmonth','indexvar','categoryTags'])['currencyAmount'].sum().reset_index()
df_txn_daily_allmerchtxn=df_txn_apr_Oct18.groupby(['yearmonth','indexvar','categoryTags'])['customerId'].count().reset_index()
df_txn_daily_allmerch.dtypes
#df_txn

df_txn_daily_allmerch=df_txn_daily_allmerch.set_index('indexvar')
df_txn_daily_allmerch.dtypes

df_txn_daily_allmerch.index

df_txn_daily_allmerch['Year'] = df_txn_daily_allmerch.index.year
df_txn_daily_allmerch['Month'] = df_txn_daily_allmerch.index.month
df_txn_daily_allmerch['Weekday Name'] = df_txn_daily_allmerch.index.weekday_name

# Identify all categories in trasaction data

#df_txn_categ=df_txn.groupby(['categoryTags'])['currencyAmount'].sum().reset_index()


# 1. Expenses by CategoryTags

Food_and_dining = df_txn_daily_allmerch.loc[df_txn_daily_allmerch['categoryTags'].isin({'Food and Dining'})]
Auto_and_Transport = df_txn_daily_allmerch.loc[df_txn_daily_allmerch['categoryTags'].isin({'Auto and Transport'})]

Bills_and_Utilities = df_txn_daily_allmerch.loc[df_txn_daily_allmerch['categoryTags'].isin({'Bills and Utilities'})]
Bills_and_Utilities_txn = df_txn_daily_allmerchtxn.loc[df_txn_daily_allmerchtxn['categoryTags'].isin({'Bills and Utilities'})]

Entertainment = df_txn_daily_allmerch.loc[df_txn_daily_allmerch['categoryTags'].isin({'Entertainment'})]
Fees_and_Charges = df_txn_daily_allmerch.loc[df_txn_daily_allmerch['categoryTags'].isin({'Fees and Charges'})]
Health_and_Fitness	= df_txn_daily_allmerch.loc[df_txn_daily_allmerch['categoryTags'].isin({'Health and Fitness'})]
Home	= df_txn_daily_allmerch.loc[df_txn_daily_allmerch['categoryTags'].isin({'Home'})]

Income	= df_txn_daily_allmerch.loc[df_txn_daily_allmerch['categoryTags'].isin({'Income'})]
Income_txn = df_txn_daily_allmerchtxn.loc[df_txn_daily_allmerchtxn['categoryTags'].isin({'Income'})]

Kids	= df_txn_daily_allmerch.loc[df_txn_daily_allmerch['categoryTags'].isin({'Kids'})]
Mortgage_and_Rent	= df_txn_daily_allmerch.loc[df_txn_daily_allmerch['categoryTags'].isin({'Mortgage and Rent'})]

Shopping = df_txn_daily_allmerch.loc[df_txn_daily_allmerch['categoryTags'].isin({'Shopping'})]
Shopping_txn = df_txn_daily_allmerchtxn.loc[df_txn_daily_allmerchtxn['categoryTags'].isin({'Shopping'})]

Taxes = df_txn_daily_allmerch.loc[df_txn_daily_allmerch['categoryTags'].isin({'Taxes'})]

Transfer = df_txn_daily_allmerch.loc[df_txn_daily_allmerch['categoryTags'].isin({'Transfer'})]
Transfer_txn = df_txn_daily_allmerchtxn.loc[df_txn_daily_allmerchtxn['categoryTags'].isin({'Transfer'})]

Travel = df_txn_daily_allmerch.loc[df_txn_daily_allmerch['categoryTags'].isin({'Travel'})]
Travel_txn = df_txn_daily_allmerchtxn.loc[df_txn_daily_allmerchtxn['categoryTags'].isin({'Travel'})]

plt.style.use('seaborn-whitegrid')


fig, ax = plt.subplots(figsize=(9,7))
#for name, ax in zip(['Food_and_dining', 'Auto_and_Transport', 'Bills_and_Utilities'], axes):
#for nm in [Food_and_dining, Auto_and_Transport, Bills_and_Utilities]:
#sns.boxplot(data=opsd_daily, x='Month', y=name, ax=ax)
#ax.set_ylabel('GWh')
#ax.set_title(name)
# Remove the automatic x-axis label from all but the bottom subplot
#if ax != axes[-1]:
#    ax.set_xlabel('')
#  ax.plot(nm['currencyAmount'], label=nm)
#  ax.set(xlabel="Date", ylabel="Amount Spent", title="Daily Spend at all merchant categories")
#  ax.xaxis.set_major_locator(mdates.WeekdayLocator(interval=1))
#  ax.xaxis.set_major_formatter(DateFormatter("%m/%d"))
#  ax.legend();
#fig, ax = plt.subplots(figsize=(9, 7), subplot_kw={'ylim': (0,1000)})

#ax.plot(Food_and_dining.index.values,Food_and_dining['currencyAmount'], '-o', color='red')
#Insights - Stable across time period 

#Food_and_dining.plot(x=Food_and_dining.index.values, y='currencyAmount', ax=ax, c='blue', legend=False, lw=2.5, ylim=(0, 1000));

#ax.plot(Auto_and_Transport.index.values,Auto_and_Transport['currencyAmount'], '-o', color='red')
#Insights - Increasing trend across time period 

#ax.plot(Bills_and_Utilities.index.values,Bills_and_Utilities['currencyAmount'], '-o', color='red')
#Insights - Stable trend across time period 

#ax.plot(Bills_and_Utilities_txn.index.values,Bills_and_Utilities_txn['customerId'], '-o', color='red')
#Insights - Stable trend of number of transactions at daily level

#ax.set(xlabel="Date", ylabel="# of Transactions", title="Txn at Bills_and_Utilities merchant categories")


#ax.plot(Entertainment.index.values,Entertainment['currencyAmount'], '-o', color='red')
#Insights - Stable trend , fewer days, across time period 

#ax.plot(Fees_and_Charges.index.values,Fees_and_Charges['currencyAmount'], '-o', color='red')
#Insights - 0 days, across time period 

#ax.plot(Health_and_Fitness.index.values,Health_and_Fitness['currencyAmount'], '-o', color='red')
#Insights - 8 days, across time period 

#ax.plot(Home.index.values,Home['currencyAmount'], '-o', color='red')
#Insights - Increasing trend across time period 

#ax.plot(Income.index.values,Income['currencyAmount'], '-o', color='red')
#Insights - 21 days across time period 

#ax.plot(Income_txn.index.values,Income_txn['customerId'], '-o', color='red')
#Insights - 21 days, Stable trend

#ax.set(xlabel="Date", ylabel="# of Transactions", title="Txn at Income merchant categories")

#ax.plot(Kids.index.values,Kids['currencyAmount'], '-o', color='red')
#Insights - 2 days across time period 

#ax.plot(Mortgage_and_Rent.index.values,Mortgage_and_Rent['currencyAmount'], '-o', color='red')
#Insights - 7 days across time period 

#ax.plot(Shopping.index.values,Shopping['currencyAmount'], '-o', color='red')
#Insights - Stable trend across time period 

#ax.plot(Shopping_txn.index.values,Shopping_txn['customerId'], '-o', color='red')
#Insights - days in range 1 to 6 , Increasing trend

#ax.set(xlabel="Date", ylabel="# of Transactions", title="Txn at Shopping merchant categories")


#ax.plot(Taxes.index.values,Taxes['currencyAmount'], '-o', color='red')
#Insights - 6 days across time period 

#ax.plot(Transfer.index.values,Transfer['currencyAmount'], '-o', color='red')
#Insights - 2 series : 100 to max 30K - Most , 18 days in 30K to 300k and 
#10 days in range 400K - 900K, slightly declining trnd for most days

ax.plot(Transfer_txn.index.values,Transfer_txn['customerId'], '-o', color='red')
#Insights - Slight declining trend

ax.set(xlabel="Daily trend", ylabel="# of Transactions", title="Txn at Transfer merchant categories")

#ax.plot(Travel.index.values,Travel['currencyAmount'], '-o', color='red')
#Insights - days in range 0 to 5K , Increasing trend

#ax.set(xlabel="Date", ylabel="Amount Spent", title="Daily Spend at Travel merchant categories")

#ax.plot(Travel_txn.index.values,Travel_txn['customerId'], '-o', color='red')
#Insights - days in range 1 to 6 , Increasing trend

#ax.set(xlabel="Date", ylabel="# of Transactions", title="Txn at Travel merchant categories")

from matplotlib.dates import DateFormatter
import matplotlib.dates as mdates
# Clean up the x axis dates
#ax.xaxis.set_major_locator(mdates.WeekdayLocator(interval=1))
ax.xaxis.set_major_locator(mdates.MonthLocator(interval=1))
ax.xaxis.set_major_formatter(DateFormatter("%m/%d"))

plt.show()

"""### Stackded bar showing spend days by merchant category"""

# Monthly distribution of customers, # days of Trnx, Amount by categories

q20 = """ select categoryTags,count(case when currencyAmount>0 then currencyAmount else NULL end) as txn,
count(distinct case when currencyAmount>0 then customerId else NULL end) as cust,
count(distinct case when currencyAmount>0 then date_YMD else NULL end) as dys,
sum(currencyAmount) as amt
from df_txn where (yearmonth in ('201807')) 
group by categoryTags
order by cust desc"""
print(psql.sqldf(q20,locals()))

# Stacked bar on number of days

# For number of days of trnx per month : df_txn_daily_allmerch

table1=pd.crosstab(index=df_txn_daily_allmerch.yearmonth, columns=df_txn_daily_allmerch.categoryTags)
table1.div(table1.sum(1).astype(float), axis=0).plot(
    kind='bar', stacked=True, figsize=(8,6))
plt.title('Stacked Bar Chart of year Month vs # Spend Days')
plt.xlabel('# Year Month')
plt.ylabel('Proportion of # Spend Days by Category')
plt.legend(bbox_to_anchor=(1.0,0.8), loc="upper left",ncol=2)

table1

#transfer_df = df_txn.loc[df_txn['categoryTags'].isin({'Transfer'})].reset_index()
#transfer_df
# Merchants like Email Transfer, Trnsfer to C/C , Etransfr 

#Income_df = df_txn.loc[df_txn['categoryTags'].isin({'Income'})].reset_index()
#Income_df
# Merchants like ADP PMT, CANADA CPP, 

# Transfer (3.3k cust, $1.8M/mth, 30 days/mth) or Income (1.8k cust, $2.8M/mth, 3 days/mth) or Bills and Utilities (1.7k cust, $200K/mth, 21 days/mth) or Food and Dining (430 cust, $9K/mth, 21 days/mth) or Taxes(340 cust, $340k/mth, 1 day/mth)
# New product Category - Transfer (3.3k cust, $1.8M/mth, 30 days/mth) - High frequency, high volume , large customer base

# Obtaining distribution of Income

df_custs.totalIncome.describe()
df_custs.totalIncome.quantile(np.linspace(.1, 1, 9, 0))

# Obtaining distribution of Balance by account type

df_custs_accts.loc[(df_custs_accts['type_y'].isin({'DDA'}))].balance.describe()
df_custs_accts.loc[(df_custs_accts['type_y'].isin({'DDA'}))].balance.quantile(np.linspace(.1, 1, 9, 0))

# DDA account type has negative balances

df_custs_accts.loc[(df_custs_accts['type_y'].isin({'SDA'}))].balance.describe()
#df_custs_accts.loc[(df_custs_accts['type_y'].isin({'SDA'}))].balance.quantile(np.linspace(.1, 1, 9, 0))

# SDA account type does not have negative balances - Higher balance accounts

"""### Creating new Variables for Segmentation"""

# Creating flags for Binned variables from Demographic information

def finc(x1):
    if (x1 <= 12800):
        return '1'
    elif (x1 > 12800) and (x1 <= 28700):
        return '2'
    elif (x1 > 28700) and (x1 <= 48400):
        return '3'
    elif (x1 > 48400) and (x1 <=79500):
        return '4'
    elif (x1 > 79500):
        return '5'

df_custs_accts['incomebins'] = df_custs_accts['totalIncome'].apply(finc)

q182 = """ select incomebins,
min(totalIncome) as min_inc,
max(totalIncome) as max_inc
from df_custs_accts group by incomebins """
print(psql.sqldf(q182,locals()))

df_custs_accts['workActivity'] = df_custs_accts['workActivity'].fillna(0)

workbins = {'fulltime' : '1', 'parttime' : '2', 0 : '3'}

df_custs_accts['work_bins'] = df_custs_accts['workActivity'].map(workbins)

q184 = """ select work_bins,workActivity,
count(distinct(id_x)) as cust_count
from df_custs_accts group by work_bins,workActivity """
print(psql.sqldf(q184,locals()))

df_custs_accts['habitationStatus'] = df_custs_accts['habitationStatus'].fillna(0)

hbtbins = {'Group' : '1', 'With Parent' : '2', 'With Spouse' : '3' , 0 : '4'}

df_custs_accts['hbtstatus_bins'] = df_custs_accts['habitationStatus'].map(hbtbins)

q185 = """ select hbtstatus_bins,habitationStatus,
count(distinct(id_x)) as cust_count
from df_custs_accts group by hbtstatus_bins,habitationStatus """
print(psql.sqldf(q185,locals()))

df_custs_accts['Schools_bins'] = df_custs_accts['schools'].fillna(0)

q186 = """ select Schools_bins,schools,
count(distinct(id_x)) as cust_count
from df_custs_accts group by Schools_bins,schools """
print(psql.sqldf(q186,locals()))

# Creating behavioural variables from customers' transactional data

df_txn_cust_uniqcateg = df_txn_apr_Oct18[df_txn_apr_Oct18['merchcateg_txnflag']==1].groupby('customerId')['categoryTags'].nunique().reset_index()

df_txn_cust_uniqmth = df_txn_apr_Oct18[df_txn_apr_Oct18['merchcateg_txnflag']==1].groupby('customerId')['yearmonth'].nunique().reset_index()
#df_txn_cust_uniqmth

df_txn_cust_mrntxn = df_txn_apr_Oct18.loc[(df_txn_apr_Oct18['merchcateg_txnflag'].isin({'1'})) & (df_txn_apr_Oct18['session1'].isin({'1'}))].groupby('customerId')['currencyAmount'].count().reset_index()
# 439 customers
df_txn_cust_noontxn = df_txn_apr_Oct18.loc[(df_txn_apr_Oct18['merchcateg_txnflag'].isin({'1'})) & (df_txn_apr_Oct18['session1'].isin({'2'}))].groupby('customerId')['currencyAmount'].count().reset_index()
# 439 customers
df_txn_cust_evetxn = df_txn_apr_Oct18.loc[(df_txn_apr_Oct18['merchcateg_txnflag'].isin({'1'})) & (df_txn_apr_Oct18['session1'].isin({'3'}))].groupby('customerId')['currencyAmount'].count().reset_index()
# 435 customers 
df_txn_cust_nghttxn = df_txn_apr_Oct18.loc[(df_txn_apr_Oct18['merchcateg_txnflag'].isin({'1'})) & (df_txn_apr_Oct18['session1'].isin({'4'}))].groupby('customerId')['currencyAmount'].count().reset_index()
# 423 customers 
df_txn_cust_ltnghttxn = df_txn_apr_Oct18.loc[(df_txn_apr_Oct18['merchcateg_txnflag'].isin({'1'})) & (df_txn_apr_Oct18['session1'].isin({'5'}))].groupby('customerId')['currencyAmount'].count().reset_index()
# 5,473 customers 


# Overall spend and transactions 

df_txn_cust_alltxn = df_txn_apr_Oct18.loc[(df_txn_apr_Oct18['merchcateg_txnflag'].isin({'1'}))].groupby('customerId')['currencyAmount'].count().reset_index() 
df_txn_cust_allamt = df_txn_apr_Oct18.loc[(df_txn_apr_Oct18['merchcateg_txnflag'].isin({'1'}))].groupby('customerId')['currencyAmount'].sum().reset_index() 
df_txn_cust_allamt3mth = df_txn_apr_Oct18.loc[(df_txn_apr_Oct18['yearmonth'].isin({'201808','201809','201810'}))  & (df_txn_apr_Oct18['merchcateg_txnflag'].isin({'1'}))].groupby('customerId')['currencyAmount'].sum().reset_index() 
df_txn_cust_allamt3b3mth = df_txn_apr_Oct18.loc[(df_txn_apr_Oct18['yearmonth'].isin({'201805','201806','201807'}))  & (df_txn_apr_Oct18['merchcateg_txnflag'].isin({'1'}))].groupby('customerId')['currencyAmount'].sum().reset_index() 


#Transfer category spend and transactions - 5277

df_txn_cust_trnsfrtxn = df_txn_apr_Oct18.loc[(df_txn_apr_Oct18['categoryTags'].isin({'Transfer'})) & (df_txn_apr_Oct18['merchcateg_txnflag'].isin({'1'}))].groupby('customerId')['currencyAmount'].count().reset_index() 
df_txn_cust_trnsframt = df_txn_apr_Oct18.loc[(df_txn_apr_Oct18['categoryTags'].isin({'Transfer'})) & (df_txn_apr_Oct18['merchcateg_txnflag'].isin({'1'}))].groupby('customerId')['currencyAmount'].sum().reset_index() 
df_txn_cust_trnsframt3mth = df_txn_apr_Oct18.loc[(df_txn_apr_Oct18['categoryTags'].isin({'Transfer'})) & (df_txn_apr_Oct18['yearmonth'].isin({'201808','201809','201810'}))  & (df_txn_apr_Oct18['merchcateg_txnflag'].isin({'1'}))].groupby('customerId')['currencyAmount'].sum().reset_index() 
df_txn_cust_trnsframt3b3mth = df_txn_apr_Oct18.loc[(df_txn_apr_Oct18['categoryTags'].isin({'Transfer'})) & (df_txn_apr_Oct18['yearmonth'].isin({'201805','201806','201807'}))  & (df_txn_apr_Oct18['merchcateg_txnflag'].isin({'1'}))].groupby('customerId')['currencyAmount'].sum().reset_index() 
#df_txn_apr_Oct18[df_txn_apr_Oct18[(['merchcateg_txnflag']==1) & (['categoryTags']=='Transfer')]].groupby('customerId')['currencyAmount'].count().reset_index()
df_txn_cust_trnsframt

#Income category spend and transactions - 3966 customers

df_txn_cust_incmtxn = df_txn_apr_Oct18.loc[(df_txn_apr_Oct18['categoryTags'].isin({'Income'})) & (df_txn_apr_Oct18['merchcateg_txnflag'].isin({'1'}))].groupby('customerId')['currencyAmount'].count().reset_index() 
df_txn_cust_incmamt = df_txn_apr_Oct18.loc[(df_txn_apr_Oct18['categoryTags'].isin({'Income'})) & (df_txn_apr_Oct18['merchcateg_txnflag'].isin({'1'}))].groupby('customerId')['currencyAmount'].sum().reset_index()
df_txn_cust_incmamt3mth = df_txn_apr_Oct18.loc[(df_txn_apr_Oct18['categoryTags'].isin({'Income'})) & (df_txn_apr_Oct18['yearmonth'].isin({'201808','201809','201810'})) & (df_txn_apr_Oct18['merchcateg_txnflag'].isin({'1'}))].groupby('customerId')['currencyAmount'].sum().reset_index() 
df_txn_cust_incmamt3b3mth = df_txn_apr_Oct18.loc[(df_txn_apr_Oct18['categoryTags'].isin({'Income'})) & (df_txn_apr_Oct18['yearmonth'].isin({'201805','201806','201807'})) & (df_txn_apr_Oct18['merchcateg_txnflag'].isin({'1'}))].groupby('customerId')['currencyAmount'].sum().reset_index() 
#df_txn_apr_Oct18[df_txn_apr_Oct18[(['merchcateg_txnflag']==1) & (['categoryTags']=='Transfer')]].groupby('customerId')['currencyAmount'].count().reset_index()
df_txn_cust_incmamt

#Bills and Utilities category spend and transactions - 3,132 customers

df_txn_cust_butiltxn = df_txn_apr_Oct18.loc[(df_txn_apr_Oct18['categoryTags'].isin({'Bills and Utilities'})) & (df_txn_apr_Oct18['merchcateg_txnflag'].isin({'1'}))].groupby('customerId')['currencyAmount'].count().reset_index() 
df_txn_cust_butilamt = df_txn_apr_Oct18.loc[(df_txn_apr_Oct18['categoryTags'].isin({'Bills and Utilities'})) & (df_txn_apr_Oct18['merchcateg_txnflag'].isin({'1'}))].groupby('customerId')['currencyAmount'].sum().reset_index() 
df_txn_cust_butilamt3mth = df_txn_apr_Oct18.loc[(df_txn_apr_Oct18['categoryTags'].isin({'Bills and Utilities'})) & (df_txn_apr_Oct18['yearmonth'].isin({'201808','201809','201810'})) & (df_txn_apr_Oct18['merchcateg_txnflag'].isin({'1'}))].groupby('customerId')['currencyAmount'].sum().reset_index() 
df_txn_cust_butilamt3b3mth = df_txn_apr_Oct18.loc[(df_txn_apr_Oct18['categoryTags'].isin({'Bills and Utilities'})) & (df_txn_apr_Oct18['yearmonth'].isin({'201805','201806','201807'})) & (df_txn_apr_Oct18['merchcateg_txnflag'].isin({'1'}))].groupby('customerId')['currencyAmount'].sum().reset_index() 
#df_txn_apr_Oct18[df_txn_apr_Oct18[(['merchcateg_txnflag']==1) & (['categoryTags']=='Transfer')]].groupby('customerId')['currencyAmount'].count().reset_index()
df_txn_cust_butilamt

#Food and Dining category spend and transactions - 439 customers

df_txn_cust_fddintxn = df_txn_apr_Oct18.loc[(df_txn_apr_Oct18['categoryTags'].isin({'Food and Dining'})) & (df_txn_apr_Oct18['merchcateg_txnflag'].isin({'1'}))].groupby('customerId')['currencyAmount'].count().reset_index() 
df_txn_cust_fddinamt = df_txn_apr_Oct18.loc[(df_txn_apr_Oct18['categoryTags'].isin({'Food and Dining'})) & (df_txn_apr_Oct18['merchcateg_txnflag'].isin({'1'}))].groupby('customerId')['currencyAmount'].sum().reset_index() 
df_txn_cust_fddinamt3mth = df_txn_apr_Oct18.loc[(df_txn_apr_Oct18['categoryTags'].isin({'Food and Dining'})) & (df_txn_apr_Oct18['yearmonth'].isin({'201808','201809','201810'})) & (df_txn_apr_Oct18['merchcateg_txnflag'].isin({'1'}))].groupby('customerId')['currencyAmount'].sum().reset_index() 
df_txn_cust_fddinamt3b3mth = df_txn_apr_Oct18.loc[(df_txn_apr_Oct18['categoryTags'].isin({'Food and Dining'})) & (df_txn_apr_Oct18['yearmonth'].isin({'201805','201806','201807'})) & (df_txn_apr_Oct18['merchcateg_txnflag'].isin({'1'}))].groupby('customerId')['currencyAmount'].sum().reset_index() 
#df_txn_apr_Oct18[df_txn_apr_Oct18[(['merchcateg_txnflag']==1) & (['categoryTags']=='Transfer')]].groupby('customerId')['currencyAmount'].count().reset_index()
df_txn_cust_fddinamt

#Mortgage and Rent category spend and transactions - 486 customers

df_txn_cust_mtrrnttxn = df_txn_apr_Oct18.loc[(df_txn_apr_Oct18['categoryTags'].isin({'Mortgage and Rent'})) & (df_txn_apr_Oct18['merchcateg_txnflag'].isin({'1'}))].groupby('customerId')['currencyAmount'].count().reset_index() 
df_txn_cust_mtrrntamt = df_txn_apr_Oct18.loc[(df_txn_apr_Oct18['categoryTags'].isin({'Mortgage and Rent'})) & (df_txn_apr_Oct18['merchcateg_txnflag'].isin({'1'}))].groupby('customerId')['currencyAmount'].sum().reset_index() 
df_txn_cust_mtrrntamt3mth = df_txn_apr_Oct18.loc[(df_txn_apr_Oct18['categoryTags'].isin({'Mortgage and Rent'})) & (df_txn_apr_Oct18['yearmonth'].isin({'201808','201809','201810'})) & (df_txn_apr_Oct18['merchcateg_txnflag'].isin({'1'}))].groupby('customerId')['currencyAmount'].sum().reset_index() 
df_txn_cust_mtrrntamt3b3mth = df_txn_apr_Oct18.loc[(df_txn_apr_Oct18['categoryTags'].isin({'Mortgage and Rent'})) & (df_txn_apr_Oct18['yearmonth'].isin({'201805','201806','201807'})) & (df_txn_apr_Oct18['merchcateg_txnflag'].isin({'1'}))].groupby('customerId')['currencyAmount'].sum().reset_index() 
#df_txn_apr_Oct18[df_txn_apr_Oct18[(['merchcateg_txnflag']==1) & (['categoryTags']=='Transfer')]].groupby('customerId')['currencyAmount'].count().reset_index()
df_txn_cust_mtrrntamt

#Home category spend and transactions - 502 customers

df_txn_cust_Hometxn = df_txn_apr_Oct18.loc[(df_txn_apr_Oct18['categoryTags'].isin({'Home'})) & (df_txn_apr_Oct18['merchcateg_txnflag'].isin({'1'}))].groupby('customerId')['currencyAmount'].count().reset_index() 
df_txn_cust_Homeamt = df_txn_apr_Oct18.loc[(df_txn_apr_Oct18['categoryTags'].isin({'Home'})) & (df_txn_apr_Oct18['merchcateg_txnflag'].isin({'1'}))].groupby('customerId')['currencyAmount'].sum().reset_index() 
df_txn_cust_Homeamt3mth = df_txn_apr_Oct18.loc[(df_txn_apr_Oct18['categoryTags'].isin({'Home'})) & (df_txn_apr_Oct18['yearmonth'].isin({'201808','201809','201810'})) & (df_txn_apr_Oct18['merchcateg_txnflag'].isin({'1'}))].groupby('customerId')['currencyAmount'].sum().reset_index() 
df_txn_cust_Homeamt3b3mth = df_txn_apr_Oct18.loc[(df_txn_apr_Oct18['categoryTags'].isin({'Home'})) & (df_txn_apr_Oct18['yearmonth'].isin({'201805','201806','201807'})) & (df_txn_apr_Oct18['merchcateg_txnflag'].isin({'1'}))].groupby('customerId')['currencyAmount'].sum().reset_index() 
#df_txn_apr_Oct18[df_txn_apr_Oct18[(['merchcateg_txnflag']==1) & (['categoryTags']=='Transfer')]].groupby('customerId')['currencyAmount'].count().reset_index()
df_txn_cust_Homeamt

#Taxes category spend and transactions - 877 customers

df_txn_cust_Txstxn = df_txn_apr_Oct18.loc[(df_txn_apr_Oct18['categoryTags'].isin({'Taxes'})) & (df_txn_apr_Oct18['merchcateg_txnflag'].isin({'1'}))].groupby('customerId')['currencyAmount'].count().reset_index() 
df_txn_cust_Txsamt = df_txn_apr_Oct18.loc[(df_txn_apr_Oct18['categoryTags'].isin({'Taxes'})) & (df_txn_apr_Oct18['merchcateg_txnflag'].isin({'1'}))].groupby('customerId')['currencyAmount'].sum().reset_index() 
df_txn_cust_Txsamt3mth = df_txn_apr_Oct18.loc[(df_txn_apr_Oct18['categoryTags'].isin({'Taxes'})) & (df_txn_apr_Oct18['yearmonth'].isin({'201808','201809','201810'})) & (df_txn_apr_Oct18['merchcateg_txnflag'].isin({'1'}))].groupby('customerId')['currencyAmount'].sum().reset_index() 
df_txn_cust_Txsamt3b3mth = df_txn_apr_Oct18.loc[(df_txn_apr_Oct18['categoryTags'].isin({'Taxes'})) & (df_txn_apr_Oct18['yearmonth'].isin({'201805','201806','201807'})) & (df_txn_apr_Oct18['merchcateg_txnflag'].isin({'1'}))].groupby('customerId')['currencyAmount'].sum().reset_index() 
#df_txn_apr_Oct18[df_txn_apr_Oct18[(['merchcateg_txnflag']==1) & (['categoryTags']=='Transfer')]].groupby('customerId')['currencyAmount'].count().reset_index()
df_txn_cust_Txsamt

#Shopping category spend and transactions - 687 customers

df_txn_cust_shptxn = df_txn_apr_Oct18.loc[(df_txn_apr_Oct18['categoryTags'].isin({'Shopping'})) & (df_txn_apr_Oct18['merchcateg_txnflag'].isin({'1'}))].groupby('customerId')['currencyAmount'].count().reset_index() 
df_txn_cust_shpamt = df_txn_apr_Oct18.loc[(df_txn_apr_Oct18['categoryTags'].isin({'Shopping'})) & (df_txn_apr_Oct18['merchcateg_txnflag'].isin({'1'}))].groupby('customerId')['currencyAmount'].sum().reset_index() 
df_txn_cust_shpamt3mth = df_txn_apr_Oct18.loc[(df_txn_apr_Oct18['categoryTags'].isin({'Shopping'})) & (df_txn_apr_Oct18['yearmonth'].isin({'201808','201809','201810'})) & (df_txn_apr_Oct18['merchcateg_txnflag'].isin({'1'}))].groupby('customerId')['currencyAmount'].sum().reset_index() 
df_txn_cust_shpamt3b3mth = df_txn_apr_Oct18.loc[(df_txn_apr_Oct18['categoryTags'].isin({'Shopping'})) & (df_txn_apr_Oct18['yearmonth'].isin({'201805','201806','201807'})) & (df_txn_apr_Oct18['merchcateg_txnflag'].isin({'1'}))].groupby('customerId')['currencyAmount'].sum().reset_index() 
#df_txn_apr_Oct18[df_txn_apr_Oct18[(['merchcateg_txnflag']==1) & (['categoryTags']=='Transfer')]].groupby('customerId')['currencyAmount'].count().reset_index()
df_txn_cust_shpamt

#Other category - Entertainment, Travel, Auto and Transport, Health and Fitness, Fees and Charges, Kids - 169 customers

df_txn_cust_othrtxn = df_txn_apr_Oct18.loc[(df_txn_apr_Oct18['categoryTags'].isin({'Entertainment','Travel','Auto and Transport','Health and Fitness','Kids','Fees and Charges'})) & (df_txn_apr_Oct18['merchcateg_txnflag'].isin({'1'}))].groupby('customerId')['currencyAmount'].count().reset_index() 
df_txn_cust_othramt = df_txn_apr_Oct18.loc[(df_txn_apr_Oct18['categoryTags'].isin({'Entertainment','Travel','Auto and Transport','Health and Fitness','Kids','Fees and Charges'})) & (df_txn_apr_Oct18['merchcateg_txnflag'].isin({'1'}))].groupby('customerId')['currencyAmount'].sum().reset_index() 
df_txn_cust_othramt3mth = df_txn_apr_Oct18.loc[(df_txn_apr_Oct18['categoryTags'].isin({'Entertainment','Travel','Auto and Transport','Health and Fitness','Kids','Fees and Charges'})) & (df_txn_apr_Oct18['yearmonth'].isin({'201808','201809','201810'})) & (df_txn_apr_Oct18['merchcateg_txnflag'].isin({'1'}))].groupby('customerId')['currencyAmount'].sum().reset_index() 
df_txn_cust_othramt3b3mth = df_txn_apr_Oct18.loc[(df_txn_apr_Oct18['categoryTags'].isin({'Entertainment','Travel','Auto and Transport','Health and Fitness','Kids','Fees and Charges'})) & (df_txn_apr_Oct18['yearmonth'].isin({'201805','201806','201807'})) & (df_txn_apr_Oct18['merchcateg_txnflag'].isin({'1'}))].groupby('customerId')['currencyAmount'].sum().reset_index() 
#df_txn_apr_Oct18[df_txn_apr_Oct18[(['merchcateg_txnflag']==1) & (['categoryTags']=='Transfer')]].groupby('customerId')['currencyAmount'].count().reset_index()
df_txn_cust_othramt

#Entertainment category spend and transactions - 95 customers

df_txn_cust_entrtxn = df_txn_apr_Oct18.loc[(df_txn_apr_Oct18['categoryTags'].isin({'Entertainment'})) & (df_txn_apr_Oct18['merchcateg_txnflag'].isin({'1'}))].groupby('customerId')['currencyAmount'].count().reset_index() 
df_txn_cust_entramt = df_txn_apr_Oct18.loc[(df_txn_apr_Oct18['categoryTags'].isin({'Entertainment'})) & (df_txn_apr_Oct18['merchcateg_txnflag'].isin({'1'}))].groupby('customerId')['currencyAmount'].sum().reset_index() 
#df_txn_apr_Oct18[df_txn_apr_Oct18[(['merchcateg_txnflag']==1) & (['categoryTags']=='Transfer')]].groupby('customerId')['currencyAmount'].count().reset_index()
df_txn_cust_entramt


#Travel category spend and transactions - 82 customers

df_txn_cust_trvltxn = df_txn_apr_Oct18.loc[(df_txn_apr_Oct18['categoryTags'].isin({'Travel'})) & (df_txn_apr_Oct18['merchcateg_txnflag'].isin({'1'}))].groupby('customerId')['currencyAmount'].count().reset_index() 
df_txn_cust_trvlamt = df_txn_apr_Oct18.loc[(df_txn_apr_Oct18['categoryTags'].isin({'Travel'})) & (df_txn_apr_Oct18['merchcateg_txnflag'].isin({'1'}))].groupby('customerId')['currencyAmount'].sum().reset_index() 
#df_txn_apr_Oct18[df_txn_apr_Oct18[(['merchcateg_txnflag']==1) & (['categoryTags']=='Transfer')]].groupby('customerId')['currencyAmount'].count().reset_index()
df_txn_cust_trvlamt


#Auto and Transport category spend and transactions - 6 customers

df_txn_cust_atotrtxn = df_txn_apr_Oct18.loc[(df_txn_apr_Oct18['categoryTags'].isin({'Auto and Transport'})) & (df_txn_apr_Oct18['merchcateg_txnflag'].isin({'1'}))].groupby('customerId')['currencyAmount'].count().reset_index() 
df_txn_cust_atotramt = df_txn_apr_Oct18.loc[(df_txn_apr_Oct18['categoryTags'].isin({'Auto and Transport'})) & (df_txn_apr_Oct18['merchcateg_txnflag'].isin({'1'}))].groupby('customerId')['currencyAmount'].sum().reset_index() 
#df_txn_apr_Oct18[df_txn_apr_Oct18[(['merchcateg_txnflag']==1) & (['categoryTags']=='Transfer')]].groupby('customerId')['currencyAmount'].count().reset_index()
df_txn_cust_atotramt

#Health_and_Fitness category spend and transactions - 2 customers

df_txn_cust_hlthtxn = df_txn_apr_Oct18.loc[(df_txn_apr_Oct18['categoryTags'].isin({'Health and Fitness'})) & (df_txn_apr_Oct18['merchcateg_txnflag'].isin({'1'}))].groupby('customerId')['currencyAmount'].count().reset_index() 
df_txn_cust_hlthamt = df_txn_apr_Oct18.loc[(df_txn_apr_Oct18['categoryTags'].isin({'Health and Fitness'})) & (df_txn_apr_Oct18['merchcateg_txnflag'].isin({'1'}))].groupby('customerId')['currencyAmount'].sum().reset_index() 
#df_txn_apr_Oct18[df_txn_apr_Oct18[(['merchcateg_txnflag']==1) & (['categoryTags']=='Transfer')]].groupby('customerId')['currencyAmount'].count().reset_index()
df_txn_cust_hlthamt

#Fees and Charges category spend and transactions - 0 customers

df_txn_cust_feetxn = df_txn_apr_Oct18.loc[(df_txn_apr_Oct18['categoryTags'].isin({'Fees and Charges'})) & (df_txn_apr_Oct18['merchcateg_txnflag'].isin({'1'}))].groupby('customerId')['currencyAmount'].count().reset_index() 
df_txn_cust_feeamt = df_txn_apr_Oct18.loc[(df_txn_apr_Oct18['categoryTags'].isin({'Fees and Charges'})) & (df_txn_apr_Oct18['merchcateg_txnflag'].isin({'1'}))].groupby('customerId')['currencyAmount'].sum().reset_index() 
#df_txn_apr_Oct18[df_txn_apr_Oct18[(['merchcateg_txnflag']==1) & (['categoryTags']=='Transfer')]].groupby('customerId')['currencyAmount'].count().reset_index()
df_txn_cust_feeamt

#Kids category spend and transactions - 1 customers

df_txn_cust_kidtxn = df_txn_apr_Oct18.loc[(df_txn_apr_Oct18['categoryTags'].isin({'Kids'})) & (df_txn_apr_Oct18['merchcateg_txnflag'].isin({'1'}))].groupby('customerId')['currencyAmount'].count().reset_index() 
df_txn_cust_kidamt = df_txn_apr_Oct18.loc[(df_txn_apr_Oct18['categoryTags'].isin({'Kids'})) & (df_txn_apr_Oct18['merchcateg_txnflag'].isin({'1'}))].groupby('customerId')['currencyAmount'].sum().reset_index() 
#df_txn_apr_Oct18[df_txn_apr_Oct18[(['merchcateg_txnflag']==1) & (['categoryTags']=='Transfer')]].groupby('customerId')['currencyAmount'].count().reset_index()
df_txn_cust_kidamt

"""### Renaming columns in newly Created variable dataframes, joining to one customer level data"""

# Renaming Overall variables 

df_txn_cust_alltxn.rename(columns = {'currencyAmount':'all_txn_aproct18'}, inplace = True)

df_txn_cust_allamt.rename(columns = {'currencyAmount':'all_amt_aproct18'}, inplace = True)

df_txn_cust_allamt3mth.rename(columns = {'currencyAmount':'all_amt_last3mth'}, inplace = True)

df_txn_cust_allamt3b3mth.rename(columns = {'currencyAmount':'all_amt_3b3mth'}, inplace = True)
#df_custs_amt_txn50

# Renaming remaining category variables 

df_txn_cust_uniqcateg.rename(columns = {'categoryTags':'Uniq_Categ'}, inplace = True)
df_txn_cust_uniqcateg.dtypes
df_txn_cust_uniqcateg

df_txn_cust_uniqmth.dtypes
df_txn_cust_uniqmth.rename(columns = {'yearmonth':'Uniq_yrmnth'}, inplace = True)
df_txn_cust_uniqmth

df_txn_cust_mrntxn.dtypes
df_txn_cust_mrntxn.rename(columns = {'currencyAmount':'mrng_txn_count'}, inplace = True)
df_txn_cust_mrntxn

df_txn_cust_noontxn.dtypes
df_txn_cust_noontxn.rename(columns = {'currencyAmount':'noon_txn_count'}, inplace = True)
df_txn_cust_noontxn

df_txn_cust_evetxn.dtypes
df_txn_cust_evetxn.rename(columns = {'currencyAmount':'eve_txn_count'}, inplace = True)
df_txn_cust_evetxn

df_txn_cust_nghttxn.dtypes
df_txn_cust_nghttxn.rename(columns = {'currencyAmount':'nght_txn_count'}, inplace = True)
df_txn_cust_nghttxn

df_txn_cust_ltnghttxn.dtypes
df_txn_cust_ltnghttxn.rename(columns = {'currencyAmount':'ltnght_txn_count'}, inplace = True)
df_txn_cust_ltnghttxn

# Renaming transfer category variables 

df_txn_cust_trnsfrtxn
df_txn_cust_trnsfrtxn.rename(columns = {'currencyAmount':'trnsfr_txn_aproct18'}, inplace = True)
df_txn_cust_trnsfrtxn

df_txn_cust_trnsframt
df_txn_cust_trnsframt.rename(columns = {'currencyAmount':'trnsfr_amt_aproct18'}, inplace = True)
df_txn_cust_trnsframt

df_txn_cust_trnsframt3mth
df_txn_cust_trnsframt3mth.rename(columns = {'currencyAmount':'trnsfr_amt_last3mth'}, inplace = True)
df_txn_cust_trnsframt3mth

df_txn_cust_trnsframt3b3mth
df_txn_cust_trnsframt3b3mth.rename(columns = {'currencyAmount':'trnsfr_amt_3b3mth'}, inplace = True)
df_txn_cust_trnsframt3b3mth

df_txn_cust_trnsfrtxn

# Renaming Income category variables

df_txn_cust_incmtxn
df_txn_cust_incmtxn.rename(columns = {'currencyAmount':'incm_txn_aproct18'}, inplace = True)
df_txn_cust_incmtxn

df_txn_cust_incmamt
df_txn_cust_incmamt.rename(columns = {'currencyAmount':'incm_amt_aproct18'}, inplace = True)
df_txn_cust_incmamt

df_txn_cust_incmamt3mth
df_txn_cust_incmamt3mth.rename(columns = {'currencyAmount':'incm_amt_last3mth'}, inplace = True)
df_txn_cust_incmamt3mth

df_txn_cust_incmamt3b3mth
df_txn_cust_incmamt3b3mth.rename(columns = {'currencyAmount':'incm_amt_3b3mth'}, inplace = True)
df_txn_cust_incmamt3b3mth

# Ranaming Bills and Utilities

df_txn_cust_butiltxn
df_txn_cust_butiltxn.rename(columns = {'currencyAmount':'butil_txn_aproct18'}, inplace = True)
df_txn_cust_butiltxn

df_txn_cust_butilamt
df_txn_cust_butilamt.rename(columns = {'currencyAmount':'butil_amt_aproct18'}, inplace = True)
df_txn_cust_butilamt

df_txn_cust_butilamt3mth
df_txn_cust_butilamt3mth.rename(columns = {'currencyAmount':'butil_amt_last3mth'}, inplace = True)
df_txn_cust_butilamt3mth

df_txn_cust_butilamt3b3mth
df_txn_cust_butilamt3b3mth.rename(columns = {'currencyAmount':'butil_amt_3b3mth'}, inplace = True)
df_txn_cust_butilamt3b3mth

# Renaming Food and dining 

df_txn_cust_fddintxn
df_txn_cust_fddintxn.rename(columns = {'currencyAmount':'fddin_txn_aproct18'}, inplace = True)
df_txn_cust_fddintxn

df_txn_cust_fddinamt
df_txn_cust_fddinamt.rename(columns = {'currencyAmount':'fddin_amt_aproct18'}, inplace = True)
df_txn_cust_fddinamt

df_txn_cust_fddinamt3mth
df_txn_cust_fddinamt3mth.rename(columns = {'currencyAmount':'fddin_amt_last3mth'}, inplace = True)
df_txn_cust_fddinamt3mth

df_txn_cust_fddinamt3b3mth
df_txn_cust_fddinamt3b3mth.rename(columns = {'currencyAmount':'fddin_amt_3b3mth'}, inplace = True)
df_txn_cust_fddinamt3b3mth

# Renaming Taxes variables

df_txn_cust_Txstxn
df_txn_cust_Txstxn.rename(columns = {'currencyAmount':'Txs_txn_aproct18'}, inplace = True)
df_txn_cust_Txstxn

df_txn_cust_Txsamt
df_txn_cust_Txsamt.rename(columns = {'currencyAmount':'Txs_amt_aproct18'}, inplace = True)
df_txn_cust_Txsamt

df_txn_cust_Txsamt3mth
df_txn_cust_Txsamt3mth.rename(columns = {'currencyAmount':'Txs_amt_last3mth'}, inplace = True)
df_txn_cust_Txsamt3mth

df_txn_cust_Txsamt3b3mth
df_txn_cust_Txsamt3b3mth.rename(columns = {'currencyAmount':'Txs_amt_3b3mth'}, inplace = True)
df_txn_cust_Txsamt3b3mth

# Renaming Shopping variables 

df_txn_cust_shptxn
df_txn_cust_shptxn.rename(columns = {'currencyAmount':'shp_txn_aproct18'}, inplace = True)
df_txn_cust_shptxn

df_txn_cust_shpamt
df_txn_cust_shpamt.rename(columns = {'currencyAmount':'shp_amt_aproct18'}, inplace = True)
df_txn_cust_shpamt

df_txn_cust_shpamt3mth
df_txn_cust_shpamt3mth.rename(columns = {'currencyAmount':'shp_amt_last3mth'}, inplace = True)
df_txn_cust_shpamt3mth

df_txn_cust_shpamt3b3mth
df_txn_cust_shpamt3b3mth.rename(columns = {'currencyAmount':'shp_amt_3b3mth'}, inplace = True)
df_txn_cust_shpamt3b3mth


# Renaming Mortgage and Rent variables 

df_txn_cust_mtrrnttxn
df_txn_cust_mtrrnttxn.rename(columns = {'currencyAmount':'mtrrnt_txn_aproct18'}, inplace = True)
df_txn_cust_mtrrnttxn

df_txn_cust_mtrrntamt
df_txn_cust_mtrrntamt.rename(columns = {'currencyAmount':'mtrrnt_amt_aproct18'}, inplace = True)
df_txn_cust_mtrrntamt

df_txn_cust_mtrrntamt3mth
df_txn_cust_mtrrntamt3mth.rename(columns = {'currencyAmount':'mtrrnt_amt_last3mth'}, inplace = True)
df_txn_cust_mtrrntamt3mth

df_txn_cust_mtrrntamt3b3mth
df_txn_cust_mtrrntamt3b3mth.rename(columns = {'currencyAmount':'mtrrnt_amt_3b3mth'}, inplace = True)
df_txn_cust_mtrrntamt3b3mth

# Renaming Home variables 

df_txn_cust_Hometxn
df_txn_cust_Hometxn.rename(columns = {'currencyAmount':'Home_txn_aproct18'}, inplace = True)
df_txn_cust_Hometxn

df_txn_cust_Homeamt
df_txn_cust_Homeamt.rename(columns = {'currencyAmount':'Home_amt_aproct18'}, inplace = True)
df_txn_cust_Homeamt

df_txn_cust_Homeamt3mth
df_txn_cust_Homeamt3mth.rename(columns = {'currencyAmount':'Home_amt_last3mth'}, inplace = True)
df_txn_cust_Homeamt3mth

df_txn_cust_Homeamt3b3mth
df_txn_cust_Homeamt3b3mth.rename(columns = {'currencyAmount':'Home_amt_3b3mth'}, inplace = True)
df_txn_cust_Homeamt3b3mth

# Renaming Other category variables 

df_txn_cust_othrtxn
df_txn_cust_othrtxn.rename(columns = {'currencyAmount':'othr_txn_aproct18'}, inplace = True)
df_txn_cust_othrtxn

df_txn_cust_othramt
df_txn_cust_othramt.rename(columns = {'currencyAmount':'othr_amt_aproct18'}, inplace = True)
df_txn_cust_othramt

df_txn_cust_othramt3mth
df_txn_cust_othramt3mth.rename(columns = {'currencyAmount':'othr_amt_last3mth'}, inplace = True)
df_txn_cust_othramt3mth

df_txn_cust_othramt3b3mth
df_txn_cust_othramt3b3mth.rename(columns = {'currencyAmount':'othr_amt_3b3mth'}, inplace = True)
df_txn_cust_othramt3b3mth

# Joining tables to one customer level data

df_custs_amt_txn1 = pd.merge(left=df_custs,right=df_txn_cust_uniqcateg[['customerId','Uniq_Categ']], how='left', left_on='id', right_on='customerId')
df_custs_amt_txn2 = pd.merge(left=df_custs_amt_txn1,right=df_txn_cust_uniqmth, how='left', left_on='id', right_on='customerId')
df_custs_amt_txn3= pd.merge(left=df_custs_amt_txn2,right=df_txn_cust_mrntxn, how='left', left_on='id', right_on='customerId')
df_custs_amt_txn4= pd.merge(left=df_custs_amt_txn3,right=df_txn_cust_noontxn, how='left', left_on='id', right_on='customerId')
df_custs_amt_txn5= pd.merge(left=df_custs_amt_txn4,right=df_txn_cust_evetxn, how='left', left_on='id', right_on='customerId')
df_custs_amt_txn6= pd.merge(left=df_custs_amt_txn5,right=df_txn_cust_nghttxn, how='left', left_on='id', right_on='customerId')
df_custs_amt_txn7= pd.merge(left=df_custs_amt_txn6,right=df_txn_cust_ltnghttxn, how='left', left_on='id', right_on='customerId')
df_custs_amt_txn8= pd.merge(left=df_custs_amt_txn7,right=df_txn_cust_trnsfrtxn, how='left', left_on='id', right_on='customerId')
df_custs_amt_txn9= pd.merge(left=df_custs_amt_txn8,right=df_txn_cust_trnsframt, how='left', left_on='id', right_on='customerId')
df_custs_amt_txn10= pd.merge(left=df_custs_amt_txn9,right=df_txn_cust_trnsframt3mth, how='left', left_on='id', right_on='customerId')
df_custs_amt_txn11= pd.merge(left=df_custs_amt_txn10,right=df_txn_cust_trnsframt3b3mth, how='left', left_on='id', right_on='customerId')
df_custs_amt_txn12= pd.merge(left=df_custs_amt_txn11,right=df_txn_cust_incmtxn, how='left', left_on='id', right_on='customerId')
df_custs_amt_txn13= pd.merge(left=df_custs_amt_txn12,right=df_txn_cust_incmamt, how='left', left_on='id', right_on='customerId')
df_custs_amt_txn14= pd.merge(left=df_custs_amt_txn13,right=df_txn_cust_incmamt3mth, how='left', left_on='id', right_on='customerId')
df_custs_amt_txn15= pd.merge(left=df_custs_amt_txn14,right=df_txn_cust_incmamt3b3mth, how='left', left_on='id', right_on='customerId')
df_custs_amt_txn16= pd.merge(left=df_custs_amt_txn15,right=df_txn_cust_butiltxn, how='left', left_on='id', right_on='customerId')
df_custs_amt_txn17= pd.merge(left=df_custs_amt_txn16,right=df_txn_cust_butilamt, how='left', left_on='id', right_on='customerId')
df_custs_amt_txn18= pd.merge(left=df_custs_amt_txn17,right=df_txn_cust_butilamt3mth, how='left', left_on='id', right_on='customerId')
df_custs_amt_txn19= pd.merge(left=df_custs_amt_txn18,right=df_txn_cust_butilamt3b3mth, how='left', left_on='id', right_on='customerId')
df_custs_amt_txn20= pd.merge(left=df_custs_amt_txn19,right=df_txn_cust_fddintxn, how='left', left_on='id', right_on='customerId')
df_custs_amt_txn21= pd.merge(left=df_custs_amt_txn20,right=df_txn_cust_fddinamt, how='left', left_on='id', right_on='customerId')
df_custs_amt_txn22= pd.merge(left=df_custs_amt_txn21,right=df_txn_cust_fddinamt3mth, how='left', left_on='id', right_on='customerId')
df_custs_amt_txn23= pd.merge(left=df_custs_amt_txn22,right=df_txn_cust_fddinamt3b3mth, how='left', left_on='id', right_on='customerId')
df_custs_amt_txn24= pd.merge(left=df_custs_amt_txn23,right=df_txn_cust_Txstxn, how='left', left_on='id', right_on='customerId')
df_custs_amt_txn25= pd.merge(left=df_custs_amt_txn24,right=df_txn_cust_Txsamt, how='left', left_on='id', right_on='customerId')
df_custs_amt_txn26= pd.merge(left=df_custs_amt_txn25,right=df_txn_cust_Txsamt3mth, how='left', left_on='id', right_on='customerId')
df_custs_amt_txn27= pd.merge(left=df_custs_amt_txn26,right=df_txn_cust_Txsamt3b3mth, how='left', left_on='id', right_on='customerId')
df_custs_amt_txn28= pd.merge(left=df_custs_amt_txn27,right=df_txn_cust_shptxn, how='left', left_on='id', right_on='customerId')
df_custs_amt_txn29= pd.merge(left=df_custs_amt_txn28,right=df_txn_cust_shpamt, how='left', left_on='id', right_on='customerId')
df_custs_amt_txn30= pd.merge(left=df_custs_amt_txn29,right=df_txn_cust_shpamt3mth, how='left', left_on='id', right_on='customerId')
df_custs_amt_txn31= pd.merge(left=df_custs_amt_txn30,right=df_txn_cust_shpamt3b3mth, how='left', left_on='id', right_on='customerId')
df_custs_amt_txn32= pd.merge(left=df_custs_amt_txn31,right=df_txn_cust_mtrrnttxn, how='left', left_on='id', right_on='customerId')
df_custs_amt_txn33= pd.merge(left=df_custs_amt_txn32,right=df_txn_cust_mtrrntamt, how='left', left_on='id', right_on='customerId')
df_custs_amt_txn34= pd.merge(left=df_custs_amt_txn33,right=df_txn_cust_mtrrntamt3mth, how='left', left_on='id', right_on='customerId')
df_custs_amt_txn35= pd.merge(left=df_custs_amt_txn34,right=df_txn_cust_mtrrntamt3b3mth, how='left', left_on='id', right_on='customerId')
df_custs_amt_txn36= pd.merge(left=df_custs_amt_txn35,right=df_txn_cust_Hometxn, how='left', left_on='id', right_on='customerId')
df_custs_amt_txn37= pd.merge(left=df_custs_amt_txn36,right=df_txn_cust_Homeamt, how='left', left_on='id', right_on='customerId')
df_custs_amt_txn38= pd.merge(left=df_custs_amt_txn37,right=df_txn_cust_Homeamt3mth, how='left', left_on='id', right_on='customerId')
df_custs_amt_txn39= pd.merge(left=df_custs_amt_txn38,right=df_txn_cust_Homeamt3b3mth, how='left', left_on='id', right_on='customerId')
df_custs_amt_txn40= pd.merge(left=df_custs_amt_txn39,right=df_txn_cust_othrtxn, how='left', left_on='id', right_on='customerId')
df_custs_amt_txn41= pd.merge(left=df_custs_amt_txn40,right=df_txn_cust_othramt, how='left', left_on='id', right_on='customerId')
df_custs_amt_txn42= pd.merge(left=df_custs_amt_txn41,right=df_txn_cust_othramt3mth, how='left', left_on='id', right_on='customerId')
df_custs_amt_txn43= pd.merge(left=df_custs_amt_txn42,right=df_txn_cust_othramt3b3mth, how='left', left_on='id', right_on='customerId')
df_custs_amt_txn44= pd.merge(left=df_custs_amt_txn43,right=df_txn_cust_alltxn, how='left', left_on='id', right_on='customerId')
df_custs_amt_txn45= pd.merge(left=df_custs_amt_txn44,right=df_txn_cust_allamt, how='left', left_on='id', right_on='customerId')
df_custs_amt_txn46= pd.merge(left=df_custs_amt_txn45,right=df_txn_cust_allamt3mth, how='left', left_on='id', right_on='customerId')
df_custs_amt_txn47= pd.merge(left=df_custs_amt_txn46,right=df_txn_cust_allamt3b3mth, how='left', left_on='id', right_on='customerId')


df_custs_amt_txn48 = df_custs_amt_txn47.drop(['customerId','customerId_x','customerId_y'],axis=1)
df_custs_amt_txn48.replace(np.nan, 0, inplace=True)

df_custs_amt_txn49=df_custs_amt_txn48.iloc[:,[0,15,16,17,18,19,20,21,59,60,61]]

df_custs_amt_txn49['IncAmtlst3mth'] = df_custs_amt_txn49['all_amt_last3mth'] - df_custs_amt_txn49['all_amt_3b3mth']

df_custs_amt_txn50 = df_custs_amt_txn49.drop(['all_amt_3b3mth'], axis=1)

"""### K - Means Clustering - Demographic"""

from sklearn.cluster import KMeans

# Keeping only customer Id and binned demograhic variables for demographic segmentation

df_custs_accts1=df_custs_accts.iloc[:,[0,23,24,25,26]]
df_custs_accts1.rename(columns={'id_x' : 'id'},inplace=True)
df_custs_accts2 = df_custs_accts1.drop_duplicates('id')

from sklearn.cluster import KMeans
wcss = []
for k in range(1,11):
    kmeans = KMeans(n_clusters=k, init="k-means++")
    kmeans.fit(df_custs_accts2.iloc[:,2:])
    wcss.append(kmeans.inertia_)
plt.figure(figsize=(12,6))    
plt.style.use('seaborn-whitegrid')
#plt.grid()
plt.title('Elbow Method')
plt.plot(range(1,11),wcss, linewidth=2, color="red", marker ="8")
plt.xlabel("Number of Clusters")
plt.xticks(np.arange(1,11,1))
plt.ylabel("WCSS")
plt.show()

km = KMeans(n_clusters=3)
clusters = km.fit_predict(df_custs_accts2.iloc[:,2:])
df_custs_accts2["label"] = clusters
df_custs_accts2

df_custs_accts_income= pd.merge(left=df_custs_accts2,right=df_custs[['id','totalIncome','workActivity','habitationStatus','schoolAttendance']], how='left', left_on='id', right_on='id')


#q25 = """ select label, incomebins, count(distinct(id)) as cust_cnt from  df_custs_accts2 group by label,incomebins """
#print(psql.sqldf(q25,locals()))

#q26 = """ select label, work_bins, count(distinct(id)) as cust_cnt from  df_custs_accts2 group by label,work_bins """
#print(psql.sqldf(q26,locals()))

#q27 = """ select label, hbtstatus_bins, count(distinct(id)) as cust_cnt from  df_custs_accts2 group by label,hbtstatus_bins """
#print(psql.sqldf(q27,locals()))

q28 = """ select label, incomebins, work_bins, hbtstatus_bins, schools_bins, count(distinct(id)) as cust_cnt from  df_custs_accts2 group by label, incomebins, work_bins, hbtstatus_bins, schools_bins """
print(psql.sqldf(q28,locals()))

# Income distribution by customers and newly created demographic segments

df_custs_accts_income.loc[(df_custs_accts_income['label'].isin({'1'}))].totalIncome.describe()

"""### K Means Clustering - Behavioural based on transactions"""

# 386 customers with negative balance

df_custs_accts_DDAbal = df_custs_accts.loc[(df_custs_accts['type_y'].isin({'DDA'}))].groupby('id_x')['balance'].min().reset_index()
df_custs_accts_DDAbal

q30 = """ select count(distinct case when balance<0 then id_x else NULL end) as neg_bal_cust from  df_custs_accts_DDAbal """
print(psql.sqldf(q30,locals()))

# 386 validated

df_custs_accts_SDAbal = df_custs_accts.loc[(df_custs_accts['type_y'].isin({'SDA'}))].groupby('id_x')['balance'].min().reset_index()
df_custs_accts_SDAbal

q31 = """ select count(distinct case when balance<0 then id_x else NULL end) as neg_bal_cust from  df_custs_accts_SDAbal """
print(psql.sqldf(q31,locals()))

# No customers with balance Negative => Customers with negative and positive balance in different accounts

df_custs_accts_bal = pd.merge(left=df_custs_accts_SDAbal,right=df_custs_accts_DDAbal, how='left', left_on='id_x', right_on='id_x')
#df_custs_accts_bal

df_custs_accts_bal.rename(columns = {'balance_x':'SDA_bal', 'balance_y':'DDA_bal'}, inplace = True)
#df_custs_accts_bal

q32 = """ select min(DDA_bal) as min_bal, max(DDA_bal) as max_bal from  df_custs_accts_bal where DDA_bal<0 """
print(psql.sqldf(q32,locals()))

q33 = """ select min(SDA_bal) as min_bal, max(SDA_bal) as max_bal from  df_custs_accts_bal where SDA_bal>=0 """
print(psql.sqldf(q33,locals()))

df_custs_amt_txn51= pd.merge(left=df_custs_amt_txn50,right=df_custs_accts_bal, how='left', left_on='id', right_on='id_x')
df_custs_amt_txn52 = df_custs_amt_txn51.drop(['id_x'], axis = 1)

features_to_standerdize = ['Uniq_Categ', 'Uniq_yrmnth', 
                         'mrng_txn_count',	'noon_txn_count',	'eve_txn_count',	'nght_txn_count',	'ltnght_txn_count',	
                         'all_amt_aproct18', 'all_amt_last3mth', 'IncAmtlst3mth', 'SDA_bal', 'DDA_bal']

df_custs_amt_txn52[features_to_standerdize] = df_custs_amt_txn52[features_to_standerdize].apply(lambda x:(x-x.min()) / (x.max()-x.min()))

from sklearn.cluster import KMeans
wcss = []
for k in range(1,11):
    kmeans = KMeans(n_clusters=k, init="k-means++")
    kmeans.fit(df_custs_amt_txn52.iloc[:,1:])
    wcss.append(kmeans.inertia_)
plt.figure(figsize=(12,6))    
plt.style.use('seaborn-whitegrid')
plt.title('Elbow Method')
plt.plot(range(1,11),wcss, linewidth=2, color="red", marker ="8")
plt.xlabel("K Value")
plt.xticks(np.arange(1,11,1))
plt.ylabel("WCSS")
plt.show()

km = KMeans(n_clusters=4)
clusters = km.fit_predict(df_custs_amt_txn52.iloc[:,1:])
df_custs_amt_txn52["label"] = clusters
df_custs_amt_txn52

q34 = """ select label, count(distinct(id)) as cust_cnt from  df_custs_amt_txn52 group by label """
print(psql.sqldf(q34,locals()))

"""## Data Preparation for Cohort analysis by Demographic clusters"""

# Bringing demographic and behavioural clusters together

df_custs_amt_clstr= pd.merge(left=df_custs_amt_txn52,right=df_custs_accts2, how='left', left_on='id', right_on='id')
df_custs_amt_clstr
df_custs_amt_clstr.rename(columns={'label_x' : 'beh_seg', 'label_y' : 'dem_seg'},inplace=True)

q35 = """ select beh_seg,dem_seg, count(distinct(id)) as cust_cnt
 from  df_custs_amt_clstr group by beh_seg,dem_seg """
print(psql.sqldf(q35,locals()))

df_custs_amt_txn48_re = df_custs_amt_txn48

df_custs_amt_txn48_re['IncrAmtlst3mth'] = df_custs_amt_txn48_re['all_amt_last3mth'] - df_custs_amt_txn48_re['all_amt_3b3mth']
df_custs_amt_txn48_re['IncrAmtlst3mth_trnsfr'] = df_custs_amt_txn48_re['trnsfr_amt_last3mth'] - df_custs_amt_txn48_re['trnsfr_amt_3b3mth']
df_custs_amt_txn48_re['IncrAmtlst3mth_incm'] = df_custs_amt_txn48_re['incm_amt_last3mth'] - df_custs_amt_txn48_re['incm_amt_3b3mth']
df_custs_amt_txn48_re['IncrAmtlst3mth_fddin'] = df_custs_amt_txn48_re['fddin_amt_last3mth'] - df_custs_amt_txn48_re['fddin_amt_3b3mth']
df_custs_amt_txn48_re['IncrAmtlst3mth_butil'] = df_custs_amt_txn48_re['butil_amt_last3mth'] - df_custs_amt_txn48_re['butil_amt_3b3mth']

df_custs_amt_txn51_re= pd.merge(left=df_custs_amt_txn48_re,right=df_custs_accts_bal, how='left', left_on='id', right_on='id_x')

# Adding segment flags at customer level

df_custs_amt_clstr2= pd.merge(left=df_custs_amt_txn51_re,right=df_custs_amt_clstr[['id','beh_seg','dem_seg']], how='left', left_on='id', right_on='id')
df_custs_amt_clstr2

# Adding segment flags at transaction level

df_txn_level_clstr2= pd.merge(left=df_txn,right=df_custs_amt_clstr[['id','beh_seg','dem_seg']], how='left', left_on='customerId', right_on='id')
df_txn_level_clstr2

"""### Cohort Analysis - Jul 2018"""

q361 = """ select count(case when currencyAmount>0 then currencyAmount else NULL end) as txn,
count(distinct case when currencyAmount>0 then customerId else NULL end) as cust,
count(distinct case when currencyAmount>0 then date_YMD else NULL end) as dys,
count(distinct case when currencyAmount>0 then accountId else NULL end) as accts,
sum(currencyAmount) as amt
from df_txn_level_clstr2 where (yearmonth in ('201807')) """
print(psql.sqldf(q361,locals()))

q362 = """ select dem_seg,count(case when currencyAmount>0 then currencyAmount else NULL end) as txn,
count(distinct case when currencyAmount>0 then customerId else NULL end) as cust,
count(distinct case when currencyAmount>0 then date_YMD else NULL end) as dys,
sum(currencyAmount) as amt
from df_txn_level_clstr2 where (yearmonth in ('201807')) group by dem_seg """
print(psql.sqldf(q362,locals()))

q363 = """ select categoryTags,
count(case when currencyAmount>0 then currencyAmount else NULL end) as txn,
count(distinct case when currencyAmount>0 then customerId else NULL end) as cust,
count(distinct case when currencyAmount>0 then date_YMD else NULL end) as dys,
sum(currencyAmount) as amt
from df_txn_level_clstr2 where (yearmonth in ('201807')) 
group by categoryTags
order by cust desc"""
print(psql.sqldf(q363,locals()))

q364 = """ select categoryTags,
count(case when currencyAmount>0 then currencyAmount else NULL end) as txn,
count(distinct case when currencyAmount>0 then customerId else NULL end) as cust,
count(distinct case when currencyAmount>0 then date_YMD else NULL end) as dys,
sum(currencyAmount) as amt
from df_txn_level_clstr2 where (yearmonth in ('201807') and dem_seg in ('0')) 
group by categoryTags
order by cust desc"""
print(psql.sqldf(q364,locals()))

df_custs_amt_clstr2.loc[(df_custs_amt_clstr2['dem_seg'].isin({'2'}))].IncrAmtlst3mth_trnsfr.describe()

df_custs_amt_clstr2.loc[(df_custs_amt_clstr2['dem_seg'].isin({'2'}))].SDA_bal.describe()

df_custs_amt_clstr2.loc[(df_custs_amt_clstr2['dem_seg'].isin({'2'}))].DDA_bal.describe()

df_custs_amt_clstr2.IncrAmtlst3mth_trnsfr.describe()

df_custs_amt_clstr2.SDA_bal.describe()

df_custs_amt_clstr2.DDA_bal.describe()